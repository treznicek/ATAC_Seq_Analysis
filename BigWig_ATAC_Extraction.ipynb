{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcd75dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed importing!\n"
     ]
    }
   ],
   "source": [
    "import pyBigWig\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Completed importing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "179e81fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Output saved to 'Master_gene_signal.tsv'.\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# Calculate BPM for each specified region in bigwig files\n",
    "############################################\n",
    "\n",
    "# Function to read BPM values from a BigWig file\n",
    "def read_bpm_from_bigwig(bigwig_file, chrom, start, stop):\n",
    "    with pyBigWig.open(bigwig_file) as bw:\n",
    "        values = bw.values(chrom, start, stop)\n",
    "        # Handle NaN values or processing here as needed\n",
    "        return sum(values) / len(values) if values else 0\n",
    "\n",
    "# Path to your TSV file\n",
    "tsv_file = '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/DiffPeaks/OPN5vVEH_genes_filtered_fromPeaks_forAudrey.tsv'\n",
    "\n",
    "# List of BigWig files\n",
    "bigwig_files = ['/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/DiffPeaks/2160_PLX5.pval.signal.bigwig', \n",
    "                '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/DiffPeaks/2160_VEH.pval.signal.bigwig',\n",
    "                '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/DiffPeaks/2161_PLX5.pval.signal.bigwig', \n",
    "                '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/DiffPeaks/2161_VEH.pval.signal.bigwig',\n",
    "                '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/DiffPeaks/2281_PLX5.pval.signal.bigwig',\n",
    "                '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/DiffPeaks/2281_VEH.pval.signal.bigwig',\n",
    "                '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/DiffPeaks/2920_PLX5.pval.signal.bigwig',\n",
    "                '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/DiffPeaks/2920_VEH.pval.signal.bigwig']\n",
    "\n",
    "# Read TSV file and process each line\n",
    "with open(tsv_file, 'r') as infile, open('/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/DiffPeaks/OPN5vVEH_genes_with_ATAC_signal.tsv', 'w', newline='') as outfile:\n",
    "    reader = csv.reader(infile, delimiter='\\t')\n",
    "    writer = csv.writer(outfile, delimiter='\\t')\n",
    "\n",
    "    # Write the header\n",
    "    header = ['Chr', 'Start', 'End', 'Gene_Name'] + ['Sample_' + str(i+1) for i in range(len(bigwig_files))]\n",
    "    writer.writerow(header)\n",
    "\n",
    "    # Process each line of the TSV file\n",
    "    for row in reader:\n",
    "        chrom, start, stop, gene = row\n",
    "        start, stop = int(start), int(stop)\n",
    "\n",
    "        # Extract BPM values for each sample\n",
    "        bpm_values = [read_bpm_from_bigwig(bw_file, chrom, start, stop) for bw_file in bigwig_files]\n",
    "\n",
    "        # Write the updated row to the output file\n",
    "        writer.writerow(row + bpm_values)\n",
    "\n",
    "print(f\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "960a2ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average signal values written to '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/GenomeAnnotation/Master_genes_median_signals.tsv'.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Do you want the average signal across samples or individual comparisons by sample name?\n",
    "Calculate average signal here, sample by sample below.\n",
    "'''\n",
    "\n",
    "def calculate_average_signals(file_path, veh_columns, opn5_columns):\n",
    "    \"\"\"\n",
    "    Calculate average signals for specified groups of columns.\n",
    "\n",
    "    :param file_path: Path to the file with the data.\n",
    "    :param veh_columns: List of column names for VEH samples.\n",
    "    :param opn5_columns: List of column names for OPN5 samples.\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    data = pd.read_csv(file_path, delimiter='\\t')\n",
    "\n",
    "    # Calculate average signals for the specified groups\n",
    "    data['avg_VEH'] = data[veh_columns].median(axis=1)\n",
    "    data['avg_OPN5'] = data[opn5_columns].median(axis=1)\n",
    "\n",
    "    # Create a new DataFrame with required columns\n",
    "    new_data = data[['chromosome', 'start', 'stop', 'gene', 'avg_VEH', 'avg_OPN5']]\n",
    "\n",
    "    # Save the new DataFrame to a file\n",
    "    output_file = '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/GenomeAnnotation/Master_genes_median_signals.tsv'\n",
    "    new_data.to_csv(output_file, sep='\\t', index=False)\n",
    "    print(f\"Average signal values written to '{output_file}'.\")\n",
    "\n",
    "# Path to your merged peaks file\n",
    "file_path = '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/GenomeAnnotation/Master_genes_signal.tsv'\n",
    "\n",
    "# Specify the columns for VEH and OPN5 samples\n",
    "veh_columns = ['VEH_2160', 'VEH_2161', 'VEH_2281', 'VEH_2920']  # Replace with actual VEH column names\n",
    "opn5_columns = ['OPN5_2160', 'OPN5_2161', 'OPN5_2281', 'OPN5_2920']  # Replace with actual OPN5 column names\n",
    "\n",
    "calculate_average_signals(file_path, veh_columns, opn5_columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b6df987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paired fold change calculation complete. Results saved to '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/GenomeAnnotation/Master_median_fold_change.tsv'.\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# Calculate fold changes\n",
    "############################################\n",
    "\n",
    "# Load the data\n",
    "file_path = '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/GenomeAnnotation/Master_genes_median_signals.tsv'\n",
    "data = pd.read_csv(file_path, delimiter='\\t')\n",
    "\n",
    "# Specify the column pairs for Vehicle and OPN5 treatments\n",
    "# Update these with the actual column names\n",
    "vehicle_columns = ['avg_VEH']#['VEH_2160', 'VEH_2161', 'VEH_2281', 'VEH_2920']\n",
    "opn5_columns = ['avg_OPN5']#['OPN5_2160', 'OPN5_2161', 'OPN5_2281', 'OPN5_2920']\n",
    "\n",
    "# Ensure that the number of vehicle columns matches the number of OPN5 columns\n",
    "if len(vehicle_columns) != len(opn5_columns):\n",
    "    raise ValueError(\"The number of Vehicle and OPN5 samples must be the same.\")\n",
    "\n",
    "# Calculate fold change for each pair (o/v)\n",
    "fold_changes = pd.DataFrame()\n",
    "for v_col, o_col in zip(vehicle_columns, opn5_columns):\n",
    "    fold_change_col = f\"Fold_Change_{v_col}_vs_{o_col}\"\n",
    "    fold_changes[fold_change_col] = data[o_col] / data[v_col]\n",
    "    \n",
    "# Calculate fold change for each pair (v/o)\n",
    "# fold_changes = pd.DataFrame()\n",
    "# for v_col, o_col in zip(vehicle_columns, opn5_columns):\n",
    "#     fold_change_col = f\"Fold_Change_{o_col}_vs_{v_col}\"\n",
    "#     fold_changes[fold_change_col] = data[v_col] / data[o_col]\n",
    "\n",
    "# Concatenate the original data with the fold change results\n",
    "result = pd.concat([data[['chromosome', 'start', 'stop', 'gene']], fold_changes], axis=1)\n",
    "\n",
    "# Save the result to a new TSV file\n",
    "output_file = '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/GenomeAnnotation/Master_median_fold_change.tsv'\n",
    "result.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "print(f\"Paired fold change calculation complete. Results saved to '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "873cc462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5             ADAM12\n",
      "14             ITGB1\n",
      "15             ITGB1\n",
      "16             ITGB1\n",
      "17             ITGB1\n",
      "            ...     \n",
      "1196             VCP\n",
      "1202    LOC105376114\n",
      "1217          CD40LG\n",
      "1218          CD40LG\n",
      "1235         ARHGEF9\n",
      "Name: gene, Length: 183, dtype: object\n",
      "Filtered gene list written to 'filtered_genes_min_4_cutoff_2.0.txt'\n",
      "0       OLMALINC\n",
      "4         ADAM12\n",
      "5         ADAM12\n",
      "14         ITGB1\n",
      "15         ITGB1\n",
      "          ...   \n",
      "1235     ARHGEF9\n",
      "1237    FLJ44635\n",
      "1238    FLJ44635\n",
      "1239    FLJ44635\n",
      "1240     MIR374B\n",
      "Name: gene, Length: 631, dtype: object\n",
      "Filtered gene list written to 'filtered_genes_min_3_cutoff_2.0.txt'\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# Filter genes based on fold change for GO analysis\n",
    "############################################\n",
    "\n",
    "def filter_genes(file_path, cutoff, min_samples=4):\n",
    "    \"\"\"\n",
    "    Filter genes based on fold change values and output to a text file.\n",
    "\n",
    "    :param file_path: Path to the TSV file with fold change data.\n",
    "    :param cutoff: The cutoff value for fold change.\n",
    "    :param min_samples: Minimum number of samples that should meet the cutoff (default 4).\n",
    "    :return: None. Outputs results to a text file.\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    data = pd.read_csv(file_path, delimiter='\\t')\n",
    "\n",
    "    # Select only fold change columns\n",
    "    fold_change_cols = [col for col in data.columns if 'Fold_Change' in col]\n",
    "\n",
    "    # Apply the cutoff and count the number of times each gene meets the criteria\n",
    "    data['above_cutoff'] = data[fold_change_cols].ge(cutoff).sum(axis=1)\n",
    "\n",
    "    # Filter genes that meet the criteria in the specified number of samples\n",
    "    filtered_genes = data[data['above_cutoff'] >= min_samples]['gene']\n",
    "\n",
    "    print(str(filtered_genes))\n",
    "    \n",
    "    # Remove duplicates and sort\n",
    "    unique_genes = sorted(set(gene for gene in filtered_genes if isinstance(gene, str)))\n",
    "\n",
    "    # Write to a text file\n",
    "    with open(f'filtered_genes_min_{min_samples}_cutoff_{cutoff}.txt', 'w') as file:\n",
    "        for gene in unique_genes:\n",
    "            file.write(f'{gene}\\n')\n",
    "\n",
    "    print(f\"Filtered gene list written to 'filtered_genes_min_{min_samples}_cutoff_{cutoff}.txt'\")\n",
    "\n",
    "# File path to the TSV file\n",
    "file_path = '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/GenomeAnnotation/VEH_paired_fold_change_output.tsv'\n",
    "\n",
    "# Set your cutoff value here\n",
    "cutoff = 2.0  # Replace YOUR_CUTOFF_VALUE with your chosen cutoff value\n",
    "\n",
    "# Output genes meeting the cutoff in all four samples to a file\n",
    "filter_genes(file_path, cutoff, min_samples=4)\n",
    "\n",
    "# Output genes meeting the cutoff in at least three samples to a file\n",
    "filter_genes(file_path, cutoff, min_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bb4381",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Now starts the calculations for GSEA\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d8c12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find average fold change across all patients at significant Manorm peak list.\n",
    "# Master peak list -> BPM values of all samples at all those sites -> calculate average BPM across VEH and OPN5 \n",
    "# -> average FC calculation -> geom mean the average FC for RNK file, submit to easyGSEA\n",
    "\n",
    "# Currently have Master gene lists biased towards VEH and OPN5, but I need the unbiased peaks as well...\n",
    "# Commands to convert peak list to gene positions:\n",
    "\n",
    "# Grab the necessary columns if the row contains a promoter proximal peak.\n",
    "!awk -F'\\t' '$8 ~ /promoter|TSS/ {print $2\"\\t\"$3\"\\t\"$4\"\\t\"$16}' Patient_2160/Unbiased_Annotated.txt > Patient_2160/Unbiased_genes_positions.txt\n",
    "\n",
    "# Just give me the unique ones, there should be no duplicate lines\n",
    "!cat Patient_2160/Unbiased_genes_positions.txt | sort | uniq > Patient_2160/Unbiased_genes_positions_nodups.txt\n",
    "\n",
    "# Join them all together and remove duplicates\n",
    "!cat Patient_2160/Unbiased_genes_positions_nodups.txt Patient_2161/Unbiased_genes_positions_nodups.txt Patient_2281/Unbiased_genes_positions_nodups.txt Patient_2920/Unbiased_genes_positions_nodups.txt | sort | uniq > Unbiased_Master_genes_positions.txt\n",
    "\n",
    "# To fix multiple peaks being in the same promoter region, find overlap between peaks and concatanate them.\n",
    "# So if the region described by the first peak overlaps the next peak, make the end position of the next \n",
    "# peak the end position of this peak and delete that row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a1e8ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged peaks written to '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/GenomeAnnotation/Tex_OCRs_hg38/combined_output_merged_peaks.bed'.\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# Ensure all overlapping peaks are merged together to reduce overlap\n",
    "############################################\n",
    "def merge_overlapping_peaks(file_path):\n",
    "    # Load the data\n",
    "    data = pd.read_csv(file_path, delimiter='\\t', names=['chromosome', 'start', 'end', 'gene_name'])\n",
    "\n",
    "    # Sort data by chromosome and start position\n",
    "    data.sort_values(by=['chromosome', 'gene_name', 'start'], inplace=True)\n",
    "\n",
    "    # Initialize a new DataFrame for the merged peaks\n",
    "    merged_data = pd.DataFrame(columns=['chromosome', 'start', 'end', 'gene_name'])\n",
    "    current_row = None\n",
    "\n",
    "    # Iterate through the rows\n",
    "    for index, row in data.iterrows():\n",
    "        if current_row is not None and row['gene_name'] == current_row['gene_name']:\n",
    "            # Check for overlap and merge if necessary\n",
    "            if row['start'] <= current_row['end']:\n",
    "                current_row['end'] = max(current_row['end'], row['end'])\n",
    "                continue\n",
    "        # Add the previous row to the merged data\n",
    "        if current_row is not None:\n",
    "            merged_data = merged_data.append(current_row, ignore_index=True)\n",
    "        current_row = row\n",
    "\n",
    "    # Add the last row\n",
    "    merged_data = merged_data.append(current_row, ignore_index=True)\n",
    "\n",
    "    # Save the merged data to a new TSV file\n",
    "    output_file = '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/GenomeAnnotation/Master_gene_positions_merged_peaks.tsv'\n",
    "    merged_data.to_csv(output_file, sep='\\t', index=False, header=False)\n",
    "    print(f\"Merged peaks written to '{output_file}'.\")\n",
    "\n",
    "# Path to your TSV file\n",
    "file_path = '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/GenomeAnnotation/Tex_OCRs_hg38/master_OCR_signal.bed'\n",
    "merge_overlapping_peaks(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2caa70ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged peaks written to '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/DiffPeaks/merged_VEH_peaks.bed'.\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# Combine overlap on next line\n",
    "############################################\n",
    "def merge_overlapping_peaks(file_path):\n",
    "    # Load the data\n",
    "    data = pd.read_csv(file_path, delimiter='\\t', names=['chromosome', 'start', 'end'], usecols=[0, 1, 2])\n",
    "\n",
    "    # Sort data by chromosome and start position just in case\n",
    "    data.sort_values(by=['chromosome', 'start'], inplace=True)\n",
    "\n",
    "    # Initialize a new DataFrame for the merged peaks\n",
    "    merged_data = pd.DataFrame(columns=['chromosome', 'start', 'end'])\n",
    "    current_chrom = None\n",
    "    current_start = None\n",
    "    current_end = None\n",
    "\n",
    "    # Iterate through the rows\n",
    "    for index, row in data.iterrows():\n",
    "        # If this is the first row or a new chromosome, reset the current variables\n",
    "        if current_chrom is None or row['chromosome'] != current_chrom:\n",
    "            # Add the previous row to the merged data if it exists\n",
    "            if current_chrom is not None:\n",
    "                merged_data = merged_data.append({'chromosome': current_chrom, 'start': current_start, 'end': current_end}, ignore_index=True)\n",
    "            current_chrom = row['chromosome']\n",
    "            current_start = row['start']\n",
    "            current_end = row['end']\n",
    "        else:\n",
    "            # If the current row overlaps with the previous, merge them\n",
    "            if row['start'] <= current_end:\n",
    "                current_end = max(current_end, row['end'])\n",
    "            else:\n",
    "                # If it doesn't overlap, add the previous range and start a new one\n",
    "                merged_data = merged_data.append({'chromosome': current_chrom, 'start': current_start, 'end': current_end}, ignore_index=True)\n",
    "                current_start = row['start']\n",
    "                current_end = row['end']\n",
    "\n",
    "    # Add the last row\n",
    "    merged_data = merged_data.append({'chromosome': current_chrom, 'start': current_start, 'end': current_end}, ignore_index=True)\n",
    "\n",
    "    # Save the merged data to a new TSV file\n",
    "    output_file = '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/DiffPeaks/merged_VEH_peaks.bed'\n",
    "    merged_data.to_csv(output_file, sep='\\t', index=False, header=False)\n",
    "    print(f\"Merged peaks written to '{output_file}'.\")\n",
    "\n",
    "# Path to your TSV file\n",
    "file_path = '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/DiffPeaks/uniq_combined_VEH_peaks.bed'\n",
    "merge_overlapping_peaks(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d12b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now with this, return to the start and get the BPM values for these genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c2b476ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median fold change for each gene is written to '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/GenomeAnnotation/Master_median_fold_change_condensed.tsv'.\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# For GSEA, we need the fold change across the entire gene, so get the median of fold change values\n",
    "############################################\n",
    "def condense_genes(file_path):\n",
    "    \"\"\"\n",
    "    Condense each gene into a single row by calculating the median fold change value.\n",
    "\n",
    "    :param file_path: Path to the file with gene data and fold change values.\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    data = pd.read_csv(file_path, delimiter='\\t')\n",
    "\n",
    "    # Calculate the median fold change for each gene\n",
    "    median_data = data.groupby('gene').median()\n",
    "\n",
    "    # Reset index to turn the gene names back into a column\n",
    "    median_data.reset_index(inplace=True)\n",
    "\n",
    "    # Select relevant columns if necessary\n",
    "    # If you want to include other columns like chromosome, start, and end, you might need additional processing\n",
    "    median_data = median_data[['gene', 'Fold_Change_avg_VEH_vs_avg_OPN5']]\n",
    "\n",
    "    # Save the result to a new file\n",
    "    output_file = '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/GenomeAnnotation/Master_median_fold_change_condensed.tsv'\n",
    "    median_data.to_csv(output_file, sep='\\t', index=False)\n",
    "    print(f\"Median fold change for each gene is written to '{output_file}'.\")\n",
    "\n",
    "# Path to your file containing the gene data\n",
    "file_path = '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/GenomeAnnotation/Master_median_fold_change.tsv'\n",
    "condense_genes(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bec813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine unbiased, OPN5 and VEH signal files...................run them through the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c0d9cf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the file: ['gene', 'change']\n",
      "RNK file created: '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/GenomeAnnotation/Master_ATAC_genes.rnk'.\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# Make RNK file from fold change output\n",
    "############################################\n",
    "def create_rnk_file(file_path, output_file):\n",
    "    \"\"\"\n",
    "    Convert fold change data into a .rnk file for GSEA analysis.\n",
    "\n",
    "    :param file_path: Path to the file with fold change data.\n",
    "    :param output_file: Path to the output .rnk file.\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    data = pd.read_csv(file_path, delimiter='\\t')\n",
    "\n",
    "    # Print out the column names to check\n",
    "    print(\"Column names in the file:\", data.columns.tolist())\n",
    "\n",
    "    # Assuming the fold change column is named 'change'. Adjust if it has a different name.\n",
    "    fold_change_column = 'change'\n",
    "\n",
    "    # Ensure the column exists\n",
    "    if fold_change_column not in data.columns:\n",
    "        raise ValueError(f\"Column '{fold_change_column}' not found in the file.\")\n",
    "\n",
    "    # Transform fold change: log2 transformation\n",
    "    data['transformed_fc'] = np.log2(data[fold_change_column].replace(0, np.nan))\n",
    "\n",
    "    # Drop NaN values (originally 1s in fold change)\n",
    "    data.dropna(subset=['transformed_fc'], inplace=True)\n",
    "\n",
    "    # Rank genes\n",
    "    ranked_genes = data[['gene', 'transformed_fc']].sort_values(by='transformed_fc', ascending=False)\n",
    "\n",
    "    # Save to .rnk file\n",
    "    ranked_genes.to_csv(output_file, sep='\\t', index=False, header=False)\n",
    "    print(f\"RNK file created: '{output_file}'.\")\n",
    "\n",
    "# Paths to your input and output files\n",
    "file_path = '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/GenomeAnnotation/Master_median_fold_change_condensed.tsv'\n",
    "output_rnk_file = '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/GenomeAnnotation/Master_ATAC_genes.rnk'\n",
    "create_rnk_file(file_path, output_rnk_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5f8fe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# Keep only one gene by averaging sample signal between sites.\n",
    "############################################\n",
    "\n",
    "def process_duplicates(input_file, output_file):\n",
    "    # Read in the TSV file\n",
    "    df = pd.read_csv(input_file, sep='\\t')\n",
    "\n",
    "    # Group by 'gene' and calculate the mean for each sample column\n",
    "    sample_columns = ['OPN5_2160', 'VEH_2160', 'OPN5_2161', 'VEH_2161', 'OPN5_2281', 'VEH_2281', 'OPN5_2920', 'VEH_2920']\n",
    "    grouped_df = df.groupby('gene')[sample_columns].mean().reset_index()\n",
    "\n",
    "    # Merge the grouped data back with the original dataframe to get the non-sample columns\n",
    "    merged_df = df.drop(columns=sample_columns).drop_duplicates('gene').merge(grouped_df, on='gene')\n",
    "\n",
    "    # Write the results to a new TSV file\n",
    "    merged_df.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "# Usage\n",
    "input_file = '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/GenomeAnnotation/Master_genes_signal.tsv'\n",
    "output_file = '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/GenomeAnnotation/Master_genes_signal_condensed.tsv'\n",
    "process_duplicates(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fd20464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumping BPM for chr1 101236558-101236977\n",
      "/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/atac/a713b253-c16b-4125-ba5b-4ddbe1de071f/call-macs2_signal_track/shard-0/execution/2160_PLX5_S2_L001_R1_001.merged.srt.nodup.no_chrM_MT.tn5.pval.signal.bigwig signal at this locus is: 49.70158002200161\n",
      "Dumping BPM for chr1 101236558-101236977\n",
      "/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/atac/a713b253-c16b-4125-ba5b-4ddbe1de071f/call-macs2_signal_track/shard-1/execution/2160_VEH_S1_L001_R1_001.trim.merged.srt.nodup.no_chrM_MT.tn5.pval.signal.bigwig signal at this locus is: 31.45534738112321\n",
      "Dumping BPM for chr1 101236558-101236977\n",
      "/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/atac/a713b253-c16b-4125-ba5b-4ddbe1de071f/call-macs2_signal_track/shard-2/execution/2161_PLX5_S8_L001_R1_001.trim.merged.srt.nodup.no_chrM_MT.tn5.pval.signal.bigwig signal at this locus is: 63.15049863032339\n",
      "Dumping BPM for chr1 101236558-101236977\n",
      "/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/atac/a713b253-c16b-4125-ba5b-4ddbe1de071f/call-macs2_signal_track/shard-3/execution/2161_VEH_S7_L001_R1_001.trim.merged.srt.nodup.no_chrM_MT.tn5.pval.signal.bigwig signal at this locus is: 99.49108020218004\n",
      "Dumping BPM for chr1 101236558-101236977\n",
      "/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/atac/a713b253-c16b-4125-ba5b-4ddbe1de071f/call-macs2_signal_track/shard-4/execution/2281_PLX5_S4_L001_R1_001.trim.merged.srt.nodup.no_chrM_MT.tn5.pval.signal.bigwig signal at this locus is: 43.26550416104264\n",
      "Dumping BPM for chr1 101236558-101236977\n",
      "/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/atac/a713b253-c16b-4125-ba5b-4ddbe1de071f/call-macs2_signal_track/shard-5/execution/2281_VEHS3_L001_R1_001.trim.merged.srt.nodup.no_chrM_MT.tn5.pval.signal.bigwig signal at this locus is: 44.68757695059219\n",
      "Dumping BPM for chr1 101236558-101236977\n",
      "/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/atac/a713b253-c16b-4125-ba5b-4ddbe1de071f/call-macs2_signal_track/shard-6/execution/2920_PLX5_S6_L001_R1_001.trim.merged.srt.nodup.no_chrM_MT.tn5.pval.signal.bigwig signal at this locus is: 107.10883935140824\n",
      "Dumping BPM for chr1 101236558-101236977\n",
      "/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/atac/a713b253-c16b-4125-ba5b-4ddbe1de071f/call-macs2_signal_track/shard-7/execution/2920_VEH_S5_L001_R1_001.trim.merged.srt.nodup.no_chrM_MT.tn5.pval.signal.bigwig signal at this locus is: 25.002891360717626\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# Filling in some gaps:\n",
    "############################################\n",
    "\n",
    "bigwig_files = ['/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/atac/a713b253-c16b-4125-ba5b-4ddbe1de071f/call-macs2_signal_track/shard-0/execution/2160_PLX5_S2_L001_R1_001.merged.srt.nodup.no_chrM_MT.tn5.pval.signal.bigwig', \n",
    "                '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/atac/a713b253-c16b-4125-ba5b-4ddbe1de071f/call-macs2_signal_track/shard-1/execution/2160_VEH_S1_L001_R1_001.trim.merged.srt.nodup.no_chrM_MT.tn5.pval.signal.bigwig', \n",
    "                '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/atac/a713b253-c16b-4125-ba5b-4ddbe1de071f/call-macs2_signal_track/shard-2/execution/2161_PLX5_S8_L001_R1_001.trim.merged.srt.nodup.no_chrM_MT.tn5.pval.signal.bigwig',\n",
    "                '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/atac/a713b253-c16b-4125-ba5b-4ddbe1de071f/call-macs2_signal_track/shard-3/execution/2161_VEH_S7_L001_R1_001.trim.merged.srt.nodup.no_chrM_MT.tn5.pval.signal.bigwig',\n",
    "                '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/atac/a713b253-c16b-4125-ba5b-4ddbe1de071f/call-macs2_signal_track/shard-4/execution/2281_PLX5_S4_L001_R1_001.trim.merged.srt.nodup.no_chrM_MT.tn5.pval.signal.bigwig',\n",
    "                '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/atac/a713b253-c16b-4125-ba5b-4ddbe1de071f/call-macs2_signal_track/shard-5/execution/2281_VEHS3_L001_R1_001.trim.merged.srt.nodup.no_chrM_MT.tn5.pval.signal.bigwig',\n",
    "                '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/atac/a713b253-c16b-4125-ba5b-4ddbe1de071f/call-macs2_signal_track/shard-6/execution/2920_PLX5_S6_L001_R1_001.trim.merged.srt.nodup.no_chrM_MT.tn5.pval.signal.bigwig',\n",
    "                '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/atac/a713b253-c16b-4125-ba5b-4ddbe1de071f/call-macs2_signal_track/shard-7/execution/2920_VEH_S5_L001_R1_001.trim.merged.srt.nodup.no_chrM_MT.tn5.pval.signal.bigwig']\n",
    "\n",
    "# Function to read BPM values from a BigWig file\n",
    "def read_bpm_from_bigwig(bigwig_file, chrom, start, stop):\n",
    "    with pyBigWig.open(bigwig_file) as bw:\n",
    "        values = bw.values(chrom, start, stop)\n",
    "        # Handle NaN values or processing here as needed\n",
    "        return sum(values) / len(values) if values else 0\n",
    "\n",
    "\n",
    "    \n",
    "chrom = \"chr1\"\n",
    "start = 101236558\n",
    "stop = 101236977\n",
    "\n",
    "# Process each line of the TSV file\n",
    "for file in bigwig_files:\n",
    "\n",
    "    print(f\"Dumping BPM for {chrom} {start}-{stop}\")\n",
    "    # Extract BPM values for each sample\n",
    "    bpm_values = read_bpm_from_bigwig(file, chrom, start, stop)\n",
    "    print(f\"{file} signal at this locus is: {bpm_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf409029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done writing to output.tsv\n"
     ]
    }
   ],
   "source": [
    "bigwig_files = ['/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/atac/a713b253-c16b-4125-ba5b-4ddbe1de071f/call-macs2_signal_track/shard-0/execution/2160_PLX5_S2_L001_R1_001.merged.srt.nodup.no_chrM_MT.tn5.pval.signal.bigwig', \n",
    "                '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/atac/a713b253-c16b-4125-ba5b-4ddbe1de071f/call-macs2_signal_track/shard-1/execution/2160_VEH_S1_L001_R1_001.trim.merged.srt.nodup.no_chrM_MT.tn5.pval.signal.bigwig', \n",
    "                '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/atac/a713b253-c16b-4125-ba5b-4ddbe1de071f/call-macs2_signal_track/shard-2/execution/2161_PLX5_S8_L001_R1_001.trim.merged.srt.nodup.no_chrM_MT.tn5.pval.signal.bigwig',\n",
    "                '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/atac/a713b253-c16b-4125-ba5b-4ddbe1de071f/call-macs2_signal_track/shard-3/execution/2161_VEH_S7_L001_R1_001.trim.merged.srt.nodup.no_chrM_MT.tn5.pval.signal.bigwig',\n",
    "                '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/atac/a713b253-c16b-4125-ba5b-4ddbe1de071f/call-macs2_signal_track/shard-4/execution/2281_PLX5_S4_L001_R1_001.trim.merged.srt.nodup.no_chrM_MT.tn5.pval.signal.bigwig',\n",
    "                '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/atac/a713b253-c16b-4125-ba5b-4ddbe1de071f/call-macs2_signal_track/shard-5/execution/2281_VEHS3_L001_R1_001.trim.merged.srt.nodup.no_chrM_MT.tn5.pval.signal.bigwig',\n",
    "                '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/atac/a713b253-c16b-4125-ba5b-4ddbe1de071f/call-macs2_signal_track/shard-6/execution/2920_PLX5_S6_L001_R1_001.trim.merged.srt.nodup.no_chrM_MT.tn5.pval.signal.bigwig',\n",
    "                '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/atac/a713b253-c16b-4125-ba5b-4ddbe1de071f/call-macs2_signal_track/shard-7/execution/2920_VEH_S5_L001_R1_001.trim.merged.srt.nodup.no_chrM_MT.tn5.pval.signal.bigwig']\n",
    "\n",
    "\n",
    "# (gene_name, chrom, start, stop)\n",
    "regions = [\n",
    "    ('CD8A', 'chr2', 86790848, 86791100),\n",
    "    ('CCL5', 'chr17', 35880239, 35880544),\n",
    "    ('CCL4', 'chr17', 36103374, 36103986),\n",
    "    ('CD70', 'chr19', 6590854, 6591117),\n",
    "    ('PLSCR1', 'chr3', 146544453, 146544820),\n",
    "    ('FAM3C', 'chr7', 121396300, 121396612),\n",
    "    ('FAM3C', 'chr7', 121396300, 121396612),\n",
    "]\n",
    "\n",
    "# Function to read average signal from a BigWig file for given coordinates\n",
    "def read_avg_signal_from_bigwig(bigwig_file, chrom, start, stop):\n",
    "    with pyBigWig.open(bigwig_file) as bw:\n",
    "        values = bw.values(chrom, start, stop)\n",
    "        # Compute average; handle cases where values list is empty\n",
    "        return sum(values) / len(values) if values else 0\n",
    "\n",
    "# Process each region and write the results to a TSV file\n",
    "with open('/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/GenomeAnnotation/Master_Extended_GSVA_Matrix.tsv', 'w', newline='') as tsvfile:\n",
    "    tsvwriter = csv.writer(tsvfile, delimiter='\\t')\n",
    "    # Write header row with file names as column titles\n",
    "    header = ['Gene Name'] + [f'File{i+1}' for i in range(len(bigwig_files))]\n",
    "    tsvwriter.writerow(header)\n",
    "    \n",
    "    for gene_name, chrom, start, stop in regions:\n",
    "        # Initialize row with gene name\n",
    "        row = [gene_name]\n",
    "        # Append average signal from each BigWig file\n",
    "        for file in bigwig_files:\n",
    "            avg_signal = read_avg_signal_from_bigwig(file, chrom, start, stop)\n",
    "            row.append(avg_signal)\n",
    "        # Write the row to the TSV file\n",
    "        tsvwriter.writerow(row)\n",
    "\n",
    "print(\"Done writing to output.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ebb5bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Combine MAnorm file with annotations.\n",
    "########################\n",
    "\n",
    "# Load the original combined MAnorm file\n",
    "manorm_df = pd.read_csv('/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/DiffPeaks/Patient_2920/OPN5_vs_VEH_mavalues.tsv', sep='\\t')\n",
    "\n",
    "# Load the HOMER annotated file\n",
    "annotated_df = pd.read_csv('/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/DiffPeaks/Patient_2920/OPN5_vs_VEH_annotatedPeaks.txt', sep='\\t', header=0)\n",
    "\n",
    "# Optional: Strip leading/trailing whitespaces from string columns\n",
    "manorm_df['Chr'] = manorm_df['Chr'].str.strip()\n",
    "annotated_df['Chr'] = annotated_df['Chr'].str.strip()\n",
    "\n",
    "# Adjust 'Start' values if necessary\n",
    "annotated_df['Start'] = annotated_df['Start'] - 1\n",
    "\n",
    "# Convert 'Start' and 'End' to integers\n",
    "manorm_df[['Start', 'End']] = manorm_df[['Start', 'End']].astype(int)\n",
    "annotated_df[['Start', 'End']] = annotated_df[['Start', 'End']].astype(int)\n",
    "\n",
    "# Merge DataFrames\n",
    "merged_df = pd.merge(manorm_df, annotated_df[['Chr', 'Start', 'End', 'Annotation', 'Gene Name', 'Gene Description']], on=['Chr', 'Start', 'End'], how='left')\n",
    "\n",
    "# Save the merged DataFrame with annotations to a new file\n",
    "merged_df.to_csv('/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/DiffPeaks/Patient_2920/OPN5vVEH_manorm_annotated.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ac728f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Chr      Start        End  M_value       P_value  file_count  \\\n",
      "120      chr1    1206496    1206980 -1.09763  1.087271e-13           4   \n",
      "1173     chr1    7943160    7943618 -2.24342  7.581174e-32           4   \n",
      "10215    chr1  117001306  117002041 -1.00881  1.419187e-03           4   \n",
      "16500    chr1  212699064  212700845 -2.13870  5.546527e-56           4   \n",
      "18457    chr1  239718564  239719501 -1.88983  7.542237e-29           4   \n",
      "23826   chr10   70602621   70603973 -1.35716  1.527645e-42           4   \n",
      "39615   chr12    9732932    9733466 -1.08319  1.014907e-05           4   \n",
      "41953   chr12   50924877   50925031  1.03725  9.872058e-24           1   \n",
      "49892   chr13   45337621   45337771 -1.95694  5.888096e-41           1   \n",
      "56993   chr14   88023733   88024229  1.59605  5.516866e-08           4   \n",
      "77830   chr17   36089576   36090816 -1.08568  5.365882e-39           4   \n",
      "103781   chr2  113117436  113117986 -1.24289  3.339118e-03           4   \n",
      "118661  chr21   25572739   25573370 -1.33915  1.743066e-70           4   \n",
      "118822  chr21   29371510   29372016 -1.07269  1.603701e-03           4   \n",
      "128755   chr3   42148152   42148376  1.01615  1.184520e-02           2   \n",
      "129281   chr3   46369594   46370281 -1.07143  5.303363e-50           4   \n",
      "143717   chr4  121164354  121164597 -1.05143  5.577121e-03           2   \n",
      "154561   chr5  157345490  157345849 -1.48264  7.023775e-09           4   \n",
      "165895   chr6  149033447  149033755 -1.30197  3.600607e-10           4   \n",
      "185646   chr8  133046191  133046782 -1.07910  1.893912e-08           4   \n",
      "\n",
      "                         Annotation     Gene Name  \n",
      "120        promoter-TSS (NM_148901)      TNFRSF18  \n",
      "1173       promoter-TSS (NM_001561)       TNFRSF9  \n",
      "10215      promoter-TSS (NM_004258)         CD101  \n",
      "16500      promoter-TSS (NM_018664)         BATF3  \n",
      "18457      promoter-TSS (NR_103776)     CHRM3-AS2  \n",
      "23826      promoter-TSS (NM_005041)          PRF1  \n",
      "39615   promoter-TSS (NM_001267701)        CLECL1  \n",
      "41953      promoter-TSS (NM_014033)       METTL7A  \n",
      "49892      promoter-TSS (NR_002967)       SNORA31  \n",
      "56993      promoter-TSS (NR_046094)     LINC01146  \n",
      "77830    promoter-TSS (NM_002983).6          CCL3  \n",
      "103781     promoter-TSS (NM_173843)         IL1RN  \n",
      "118661     promoter-TSS (NR_030784)        MIR155  \n",
      "118822     promoter-TSS (NR_046564)     BACH1-IT2  \n",
      "128755  promoter-TSS (NM_001265610)         TRAK1  \n",
      "129281  promoter-TSS (NM_001100168)          CCR5  \n",
      "143717     promoter-TSS (NM_024873)         TNIP3  \n",
      "154561  promoter-TSS (NM_001001343)         FNDC9  \n",
      "165895     promoter-TSS (NR_134599)  LOC105378047  \n",
      "185646     promoter-TSS (NR_107002)       MIR7848  \n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "# Filtering for significantly differentially accessible promoter regions of genes\n",
    "#####################\n",
    "\n",
    "# Load the Homer signature file\n",
    "signatures_df = pd.read_csv('/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/GenomeAnnotation/Tex_OCRs_hg38/Term_Tex_annotated.txt', sep='\\t')\n",
    "\n",
    "# Filter for annotations containing 'promoter'\n",
    "signatures_promoters_df = signatures_df[signatures_df['Annotation'].str.contains('promoter', case=False)]\n",
    "\n",
    "# Extract gene names\n",
    "gene_names_promoter = signatures_promoters_df['Gene Name'].unique()\n",
    "\n",
    "# Load the fold change file\n",
    "fold_change_df = pd.read_csv('/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/DiffPeaks/avg_manorm_counts_OPN5vVEH_annot_gene.tsv', sep='\\t')\n",
    "\n",
    "# Convert 'P_value' from scientific notation to float if necessary\n",
    "fold_change_df['P_value'] = pd.to_numeric(fold_change_df['P_value'], errors='coerce')\n",
    "\n",
    "# Filter for significantly differentially expressed genes\n",
    "sig_diff_genes_df = fold_change_df[(fold_change_df['M_value'].abs() > 1) & (fold_change_df['P_value'] < 0.05)]\n",
    "\n",
    "# Find gene names that are both in the signature promoters list and significantly differentially expressed\n",
    "overlap_genes = sig_diff_genes_df[sig_diff_genes_df['Gene Name'].isin(gene_names_promoter)]\n",
    "\n",
    "# Optionally, filter for 'promoter' in the annotation as well\n",
    "overlap_genes_promoter = overlap_genes[overlap_genes['Annotation'].str.contains('promoter', case=False)]\n",
    "\n",
    "# Output to a new CSV file\n",
    "overlap_genes_promoter.to_csv('/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/DiffPeaks/avg_manorm_counts_OPN5vVEH_overlap_genes.tsv', sep='\\t', index=False)\n",
    "\n",
    "# Or simply print the result for further inspection\n",
    "print(overlap_genes_promoter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fa6f9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data written to /Zulu/timrez/Projects/Wagner_Integrator_HiC_PROseq/INTS1_Regulated_Genelist_forMeta.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Load the Excel file\n",
    "file_path = '/Zulu/timrez/Projects/Wagner_Integrator_HiC_PROseq/INTS1_Regulated_Genelist_forShare.xlsx'  # Change 'your_file.xlsx' to the path of your .xlsx file\n",
    "genes_of_interest_path = '/Zulu/timrez/Projects/Wagner_Integrator_HiC_PROseq/genes_of_interest.csv'\n",
    "output_file_path = '/Zulu/timrez/Projects/Wagner_Integrator_HiC_PROseq/INTS1_Regulated_Genelist_forMeta.xlsx'  # Path for the output file\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "df = pd.read_excel(file_path, sheet_name='DEseq_INTS1_IAA_plusvsINTS1_IAA')\n",
    "\n",
    "# # Calculate the difference between 'Stop' and 'Start'\n",
    "# df['Difference'] = df['Stop'] - df['Start']\n",
    "\n",
    "# # Filter rows where the difference is greater than 25,000\n",
    "# filtered_df = df[df['Difference'] > 25000]\n",
    "\n",
    "# Read the gene names of interest into a list\n",
    "with open(genes_of_interest_path, 'r') as file:\n",
    "    genes_of_interest = file.read().splitlines()\n",
    "\n",
    "# Filter the DataFrame to keep only rows with gene names in the genes of interest list\n",
    "filtered_df = df[df['geneName'].isin(genes_of_interest)]\n",
    "\n",
    "# Drop the 'Difference' column as it's no longer needed (optional)\n",
    "#filtered_df = filtered_df.drop(columns=['Difference'])\n",
    "\n",
    "# Write the filtered DataFrame to a new Excel file\n",
    "filtered_df.to_excel(output_file_path, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Filtered data written to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "854cb8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Read the entire Excel file\n",
    "df = pd.read_excel('/Zulu/timrez/Projects/Wagner_Integrator_HiC_PROseq/INTS1_Regulated_Genelist_forMeta.xlsx')\n",
    "\n",
    "# Step 4: Save the filtered DataFrame to a new CSV file\n",
    "df.to_csv('/Zulu/timrez/Projects/Wagner_Integrator_HiC_PROseq/INTS1_Regulated_Genelist_forMeta.csv', index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cea65e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data written to /Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/DiffPeaks/avg_manorm_counts_OPN5vVEH_annot_gene_filtered_output.tsv\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# Sorting out peaks of interest\n",
    "##########################\n",
    "\n",
    "# Path to your TSV file\n",
    "file_path = '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/DiffPeaks/avg_manorm_counts_OPN5vVEH_annot_gene.tsv'\n",
    "\n",
    "# Load the TSV file into a DataFrame\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# Step 1: Filter rows where 'file_count' > 3\n",
    "df_filtered = df[df['file_count'] > 3]\n",
    "\n",
    "# Step 2: Keep only rows with 'Annotation' containing 'promoter'\n",
    "df_filtered = df_filtered[df_filtered['Annotation'].str.contains('promoter', case=False, na=False)]\n",
    "\n",
    "# Step 3: Remove duplicates based on 'Gene_Name' while keeping the row with the largest absolute 'M_value'\n",
    "df_filtered['abs_M_value'] = df_filtered['M_value'].abs()  # Create a temporary column for absolute M_value\n",
    "df_filtered = df_filtered.sort_values('abs_M_value', ascending=False).drop_duplicates('Gene_Name').sort_index()\n",
    "df_filtered = df_filtered.drop('abs_M_value', axis=1)  # Remove the temporary column\n",
    "\n",
    "# Save the filtered DataFrame to a new TSV file\n",
    "output_file_path = '/Zulu/timrez/Projects/ElGamal_D/TCell_ATAC/ENCODE_Pipeline/Data_Analysis/DiffPeaks/avg_manorm_counts_OPN5vVEH_annot_gene_filtered_output.tsv'\n",
    "df_filtered.to_csv(output_file_path, sep='\\t', index=False)\n",
    "\n",
    "print(f\"Filtered data written to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538845f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
